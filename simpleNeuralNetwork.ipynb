{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4da50dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "# try reducing the size of the video\n",
    "\n",
    "def get_emotion_label(filename):\n",
    "    parts = filename.split('-')\n",
    "    emotion_label = int(parts[2])\n",
    "    emotions = {\n",
    "        1: \"neutral\", 2: \"calm\", 3: \"happy\", 4: \"sad\",\n",
    "        5: \"angry\", 6: \"fearful\", 7: \"disgust\", 8: \"surprised\"\n",
    "    }\n",
    "    return emotions.get(emotion_label, \"unknown\")\n",
    "\n",
    "def get_frames(video_path, output_dir, frame_rate=1, network_size=(48,48)):\n",
    "    filename = os.path.basename(video_path)\n",
    "    emotion_label = get_emotion_label(filename)\n",
    "    \n",
    "    #create directory for emotion\n",
    "    emotion_dir = os.path.join(output_dir, emotion_label)\n",
    "    os.makedirs(emotion_dir, exist_ok=True)\n",
    "    \n",
    "    #read the video\n",
    "    capture = cv2.VideoCapture(video_path)\n",
    "    frame_count=0\n",
    "    success=True\n",
    "    \n",
    "    #set frame interval based on the frame rate\n",
    "    fps = int(capture.get(cv2.CAP_PROP_FPS))\n",
    "    frame_interval = int(fps / frame_rate)\n",
    "    \n",
    "    while success:\n",
    "        success, frame = capture.read()\n",
    "        # reduce size here than put into the face detector\n",
    "        # keep the bigger size as well \n",
    "        if not success:\n",
    "            break\n",
    "        \n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "        if len(faces) > 0:\n",
    "            x, y, w, h = faces[0]  # Use the first detected face\n",
    "            face = gray_frame[y:y+h, x:x+w]  # Crop the face region\n",
    "            face_resized = cv2.resize(face, network_size)  # Resize to a standard size for the network\n",
    "        else:\n",
    "            print(\"No face detected in this frame.\")\n",
    "        #save the frame at the specified time interval\n",
    "        if frame_count % frame_interval == 0:\n",
    "            frame_filename = f\"{filename.split('.')[0]}_frame{frame_count}.jpg\"\n",
    "            frame_path = os.path.join(emotion_dir, frame_filename)\n",
    "            cv2.imwrite(frame_path, face_resized)\n",
    "            \n",
    "        frame_count += 1\n",
    "        \n",
    "    capture.release()\n",
    "    print(f\"Extracted frames from {filename} into {emotion_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e737c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_folder = \"C:\\\\Users\\\\Lewis\\\\OneDrive - University of Glasgow\\\\Year 4\\\\Facial Emotion Recognition Project\\\\Actor_01\"\n",
    "output_folder = \"C:\\\\Users\\\\Lewis\\\\OneDrive - University of Glasgow\\\\Year 4\\\\Facial Emotion Recognition Project\\\\dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d90faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import os\n",
    "\n",
    "# Function to process a single video\n",
    "def process_video(video_file):\n",
    "    video_path = os.path.join(video_folder, video_file)\n",
    "    get_frames(video_path, output_folder, frame_rate=1/3)  # Adjust frame rate as needed\n",
    "\n",
    "# Define the number of processes you want to run simultaneously\n",
    "num_processes = 4\n",
    "\n",
    "# Create a pool of worker processes\n",
    "with Pool(num_processes) as pool:\n",
    "    pool.map(process_video, [f for f in os.listdir(video_folder) if f.endswith(\".mp4\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7418c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted frames from 01-02-01-01-01-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\neutral\n",
      "Extracted frames from 01-02-01-01-01-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\neutral\n",
      "Extracted frames from 01-02-01-01-02-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\neutral\n",
      "Extracted frames from 01-02-01-01-02-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\neutral\n",
      "Extracted frames from 01-02-02-01-01-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\calm\n",
      "Extracted frames from 01-02-02-01-01-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\calm\n",
      "Extracted frames from 01-02-02-01-02-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\calm\n",
      "Extracted frames from 01-02-02-01-02-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\calm\n",
      "Extracted frames from 01-02-02-02-01-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\calm\n",
      "Extracted frames from 01-02-02-02-01-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\calm\n",
      "Extracted frames from 01-02-02-02-02-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\calm\n",
      "Extracted frames from 01-02-02-02-02-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\calm\n",
      "Extracted frames from 01-02-03-01-01-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\happy\n",
      "Extracted frames from 01-02-03-01-01-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\happy\n",
      "Extracted frames from 01-02-03-01-02-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\happy\n",
      "Extracted frames from 01-02-03-01-02-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\happy\n",
      "Extracted frames from 01-02-03-02-01-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\happy\n",
      "Extracted frames from 01-02-03-02-01-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\happy\n",
      "Extracted frames from 01-02-03-02-02-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\happy\n",
      "Extracted frames from 01-02-03-02-02-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\happy\n",
      "Extracted frames from 01-02-04-01-01-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\sad\n",
      "Extracted frames from 01-02-04-01-01-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\sad\n",
      "Extracted frames from 01-02-04-01-02-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\sad\n",
      "Extracted frames from 01-02-04-01-02-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\sad\n",
      "Extracted frames from 01-02-04-02-01-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\sad\n",
      "Extracted frames from 01-02-04-02-01-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\sad\n",
      "Extracted frames from 01-02-04-02-02-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\sad\n",
      "Extracted frames from 01-02-04-02-02-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\sad\n",
      "Extracted frames from 01-02-05-01-01-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\angry\n",
      "Extracted frames from 01-02-05-01-01-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\angry\n",
      "Extracted frames from 01-02-05-01-02-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\angry\n",
      "Extracted frames from 01-02-05-01-02-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\angry\n",
      "No face detected in this frame.\n",
      "Extracted frames from 01-02-05-02-01-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\angry\n",
      "Extracted frames from 01-02-05-02-01-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\angry\n",
      "Extracted frames from 01-02-05-02-02-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\angry\n",
      "Extracted frames from 01-02-05-02-02-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\angry\n",
      "Extracted frames from 01-02-06-01-01-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\fearful\n",
      "Extracted frames from 01-02-06-01-01-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\fearful\n",
      "Extracted frames from 01-02-06-01-02-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\fearful\n",
      "Extracted frames from 01-02-06-01-02-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\fearful\n",
      "Extracted frames from 01-02-06-02-01-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\fearful\n",
      "Extracted frames from 01-02-06-02-01-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\fearful\n",
      "Extracted frames from 01-02-06-02-02-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\fearful\n",
      "Extracted frames from 01-02-06-02-02-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\fearful\n",
      "Extracted frames from 02-02-01-01-01-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\neutral\n",
      "Extracted frames from 02-02-01-01-01-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\neutral\n",
      "Extracted frames from 02-02-01-01-02-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\neutral\n",
      "Extracted frames from 02-02-01-01-02-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\neutral\n",
      "Extracted frames from 02-02-02-01-01-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\calm\n",
      "Extracted frames from 02-02-02-01-01-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\calm\n",
      "Extracted frames from 02-02-02-01-02-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\calm\n",
      "Extracted frames from 02-02-02-01-02-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\calm\n",
      "Extracted frames from 02-02-02-02-01-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\calm\n",
      "Extracted frames from 02-02-02-02-01-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\calm\n",
      "Extracted frames from 02-02-02-02-02-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\calm\n",
      "Extracted frames from 02-02-02-02-02-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\calm\n",
      "Extracted frames from 02-02-03-01-01-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\happy\n",
      "Extracted frames from 02-02-03-01-01-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\happy\n",
      "Extracted frames from 02-02-03-01-02-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\happy\n",
      "Extracted frames from 02-02-03-01-02-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\happy\n",
      "Extracted frames from 02-02-03-02-01-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\happy\n",
      "Extracted frames from 02-02-03-02-01-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\happy\n",
      "Extracted frames from 02-02-03-02-02-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\happy\n",
      "Extracted frames from 02-02-03-02-02-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\happy\n",
      "Extracted frames from 02-02-04-01-01-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\sad\n",
      "Extracted frames from 02-02-04-01-01-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\sad\n",
      "Extracted frames from 02-02-04-01-02-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\sad\n",
      "Extracted frames from 02-02-04-01-02-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\sad\n",
      "Extracted frames from 02-02-04-02-01-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\sad\n",
      "Extracted frames from 02-02-04-02-01-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\sad\n",
      "Extracted frames from 02-02-04-02-02-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\sad\n",
      "Extracted frames from 02-02-04-02-02-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\sad\n",
      "Extracted frames from 02-02-05-01-01-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\angry\n",
      "Extracted frames from 02-02-05-01-01-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\angry\n",
      "Extracted frames from 02-02-05-01-02-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\angry\n",
      "Extracted frames from 02-02-05-01-02-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\angry\n",
      "No face detected in this frame.\n",
      "No face detected in this frame.\n",
      "Extracted frames from 02-02-05-02-01-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\angry\n",
      "Extracted frames from 02-02-05-02-01-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\angry\n",
      "Extracted frames from 02-02-05-02-02-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\angry\n",
      "Extracted frames from 02-02-05-02-02-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\angry\n",
      "Extracted frames from 02-02-06-01-01-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\fearful\n",
      "Extracted frames from 02-02-06-01-01-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\fearful\n",
      "Extracted frames from 02-02-06-01-02-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\fearful\n",
      "Extracted frames from 02-02-06-01-02-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\fearful\n",
      "Extracted frames from 02-02-06-02-01-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\fearful\n",
      "Extracted frames from 02-02-06-02-01-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\fearful\n",
      "Extracted frames from 02-02-06-02-02-01-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\fearful\n",
      "Extracted frames from 02-02-06-02-02-02-01.mp4 into C:\\Users\\Lewis\\OneDrive - University of Glasgow\\Year 4\\Facial Emotion Recognition Project\\output_folder\\fearful\n"
     ]
    }
   ],
   "source": [
    "for video_file in os.listdir(video_folder):\n",
    "    if video_file.endswith(\".mp4\"):\n",
    "        video_path = os.path.join(video_folder, video_file)\n",
    "        filename = os.path.basename(video_file)\n",
    "        emotion_label = get_emotion_label(filename)\n",
    "        get_frames(video_path, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eda660d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1), #safeguard for everything being grayscale\n",
    "    transforms.ToTensor(), #turns everything into the pytorch tensor format\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "data_dir = \"C:\\\\Users\\\\paulb\\\\OneDrive - University of Glasgow\\\\Year 4\\\\Facial Emotion Recognition Project\\\\dataset\"\n",
    "\n",
    "full_dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "\n",
    "train_size = int(0.7*len(full_dataset))\n",
    "test_size = int(0.15*len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size - test_size\n",
    "train_data, val_data, test_data = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9735d9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleEmotionNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleEmotionNN, self).__init__()\n",
    "        \n",
    "        #setting up our layers for the NN all the pixels down to the 7 emotion classes\n",
    "        self.fullyConnected1 = nn.Linear(48*48, 128)\n",
    "        self.fullyConnected2 = nn.Linear(128, 7)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 48*48)\n",
    "        x = F.relu(self.fullyConnected1(x))\n",
    "        x = self.fullyConnected2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "621334e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleEmotionNN(\n",
      "  (fullyConnected1): Linear(in_features=2304, out_features=128, bias=True)\n",
      "  (fullyConnected2): Linear(in_features=128, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SimpleEmotionNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2822cb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([32, 7])\n"
     ]
    }
   ],
   "source": [
    "# Create a random tensor with shape (batch_size, 1, 48, 48) to simulate a batch of grayscale images\n",
    "sample_input = torch.randn(32, 1, 48, 48)  # 32 images in the batch\n",
    "\n",
    "# Pass the sample input through the model\n",
    "sample_output = model(sample_input)\n",
    "print(\"Output shape:\", sample_output.shape)\n",
    "\n",
    "# from my understanding it passes this down through the netwrok to get to a mapping of 7 from the original 2304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "379f86f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af3a3809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 1.0631, Validation Loss: 1.0468\n",
      "Epoch 2/100, Training Loss: 1.0248, Validation Loss: 0.9893\n",
      "Epoch 3/100, Training Loss: 0.9871, Validation Loss: 0.9561\n",
      "Epoch 4/100, Training Loss: 0.9505, Validation Loss: 0.8957\n",
      "Epoch 5/100, Training Loss: 0.9164, Validation Loss: 0.8796\n",
      "Epoch 6/100, Training Loss: 0.8760, Validation Loss: 0.8871\n",
      "Epoch 7/100, Training Loss: 0.8516, Validation Loss: 0.8577\n",
      "Epoch 8/100, Training Loss: 0.8190, Validation Loss: 0.8235\n",
      "Epoch 9/100, Training Loss: 0.7929, Validation Loss: 0.8254\n",
      "Epoch 10/100, Training Loss: 0.7605, Validation Loss: 0.8055\n",
      "Epoch 11/100, Training Loss: 0.7345, Validation Loss: 0.7687\n",
      "Epoch 12/100, Training Loss: 0.7137, Validation Loss: 0.7481\n",
      "Epoch 13/100, Training Loss: 0.6883, Validation Loss: 0.7325\n",
      "Epoch 14/100, Training Loss: 0.6701, Validation Loss: 0.7351\n",
      "Epoch 15/100, Training Loss: 0.6410, Validation Loss: 0.7313\n",
      "Epoch 16/100, Training Loss: 0.6213, Validation Loss: 0.7141\n",
      "Epoch 17/100, Training Loss: 0.6111, Validation Loss: 0.6884\n",
      "Epoch 18/100, Training Loss: 0.5833, Validation Loss: 0.6907\n",
      "Epoch 19/100, Training Loss: 0.5709, Validation Loss: 0.6697\n",
      "Epoch 20/100, Training Loss: 0.5477, Validation Loss: 0.6536\n",
      "Epoch 21/100, Training Loss: 0.5366, Validation Loss: 0.6488\n",
      "Epoch 22/100, Training Loss: 0.5230, Validation Loss: 0.6320\n",
      "Epoch 23/100, Training Loss: 0.5070, Validation Loss: 0.6276\n",
      "Epoch 24/100, Training Loss: 0.4918, Validation Loss: 0.6123\n",
      "Epoch 25/100, Training Loss: 0.4779, Validation Loss: 0.6154\n",
      "Epoch 26/100, Training Loss: 0.4676, Validation Loss: 0.5919\n",
      "Epoch 27/100, Training Loss: 0.4485, Validation Loss: 0.5987\n",
      "Epoch 28/100, Training Loss: 0.4435, Validation Loss: 0.5903\n",
      "Epoch 29/100, Training Loss: 0.4279, Validation Loss: 0.5796\n",
      "Epoch 30/100, Training Loss: 0.4216, Validation Loss: 0.5908\n",
      "Epoch 31/100, Training Loss: 0.4073, Validation Loss: 0.5644\n",
      "Epoch 32/100, Training Loss: 0.4024, Validation Loss: 0.5440\n",
      "Epoch 33/100, Training Loss: 0.3919, Validation Loss: 0.5522\n",
      "Epoch 34/100, Training Loss: 0.3791, Validation Loss: 0.5565\n",
      "Epoch 35/100, Training Loss: 0.3738, Validation Loss: 0.5488\n",
      "Epoch 36/100, Training Loss: 0.3649, Validation Loss: 0.5502\n",
      "Epoch 37/100, Training Loss: 0.3511, Validation Loss: 0.5205\n",
      "Epoch 38/100, Training Loss: 0.3494, Validation Loss: 0.5124\n",
      "Epoch 39/100, Training Loss: 0.3453, Validation Loss: 0.5184\n",
      "Epoch 40/100, Training Loss: 0.3320, Validation Loss: 0.5064\n",
      "Epoch 41/100, Training Loss: 0.3298, Validation Loss: 0.5198\n",
      "Epoch 42/100, Training Loss: 0.3197, Validation Loss: 0.5129\n",
      "Epoch 43/100, Training Loss: 0.3120, Validation Loss: 0.5247\n",
      "Epoch 44/100, Training Loss: 0.3064, Validation Loss: 0.4927\n",
      "Epoch 45/100, Training Loss: 0.3044, Validation Loss: 0.5031\n",
      "Epoch 46/100, Training Loss: 0.2947, Validation Loss: 0.4821\n",
      "Epoch 47/100, Training Loss: 0.2925, Validation Loss: 0.5087\n",
      "Epoch 48/100, Training Loss: 0.2831, Validation Loss: 0.4971\n",
      "Epoch 49/100, Training Loss: 0.2773, Validation Loss: 0.4683\n",
      "Epoch 50/100, Training Loss: 0.2733, Validation Loss: 0.4770\n",
      "Epoch 51/100, Training Loss: 0.2688, Validation Loss: 0.4620\n",
      "Epoch 52/100, Training Loss: 0.2609, Validation Loss: 0.4714\n",
      "Epoch 53/100, Training Loss: 0.2598, Validation Loss: 0.4642\n",
      "Epoch 54/100, Training Loss: 0.2500, Validation Loss: 0.4828\n",
      "Epoch 55/100, Training Loss: 0.2456, Validation Loss: 0.4721\n",
      "Epoch 56/100, Training Loss: 0.2406, Validation Loss: 0.4567\n",
      "Epoch 57/100, Training Loss: 0.2410, Validation Loss: 0.4584\n",
      "Epoch 58/100, Training Loss: 0.2371, Validation Loss: 0.4479\n",
      "Epoch 59/100, Training Loss: 0.2357, Validation Loss: 0.4573\n",
      "Epoch 60/100, Training Loss: 0.2257, Validation Loss: 0.4502\n",
      "Epoch 61/100, Training Loss: 0.2236, Validation Loss: 0.4528\n",
      "Epoch 62/100, Training Loss: 0.2199, Validation Loss: 0.4576\n",
      "Epoch 63/100, Training Loss: 0.2162, Validation Loss: 0.4282\n",
      "Epoch 64/100, Training Loss: 0.2090, Validation Loss: 0.4505\n",
      "Epoch 65/100, Training Loss: 0.2080, Validation Loss: 0.4426\n",
      "Epoch 66/100, Training Loss: 0.2023, Validation Loss: 0.4263\n",
      "Epoch 67/100, Training Loss: 0.2010, Validation Loss: 0.4260\n",
      "Epoch 68/100, Training Loss: 0.1986, Validation Loss: 0.4266\n",
      "Epoch 69/100, Training Loss: 0.1952, Validation Loss: 0.4145\n",
      "Epoch 70/100, Training Loss: 0.1931, Validation Loss: 0.4242\n",
      "Epoch 71/100, Training Loss: 0.1885, Validation Loss: 0.4201\n",
      "Epoch 72/100, Training Loss: 0.1820, Validation Loss: 0.4278\n",
      "Epoch 73/100, Training Loss: 0.1851, Validation Loss: 0.4280\n",
      "Epoch 74/100, Training Loss: 0.1783, Validation Loss: 0.4178\n",
      "Epoch 75/100, Training Loss: 0.1773, Validation Loss: 0.4255\n",
      "Epoch 76/100, Training Loss: 0.1740, Validation Loss: 0.4039\n",
      "Epoch 77/100, Training Loss: 0.1693, Validation Loss: 0.4008\n",
      "Epoch 78/100, Training Loss: 0.1685, Validation Loss: 0.4029\n",
      "Epoch 79/100, Training Loss: 0.1696, Validation Loss: 0.3983\n",
      "Epoch 80/100, Training Loss: 0.1618, Validation Loss: 0.3905\n",
      "Epoch 81/100, Training Loss: 0.1598, Validation Loss: 0.4159\n",
      "Epoch 82/100, Training Loss: 0.1603, Validation Loss: 0.3997\n",
      "Epoch 83/100, Training Loss: 0.1555, Validation Loss: 0.3820\n",
      "Epoch 84/100, Training Loss: 0.1569, Validation Loss: 0.3961\n",
      "Epoch 85/100, Training Loss: 0.1518, Validation Loss: 0.3804\n",
      "Epoch 86/100, Training Loss: 0.1490, Validation Loss: 0.3982\n",
      "Epoch 87/100, Training Loss: 0.1474, Validation Loss: 0.3883\n",
      "Epoch 88/100, Training Loss: 0.1452, Validation Loss: 0.3711\n",
      "Epoch 89/100, Training Loss: 0.1420, Validation Loss: 0.3764\n",
      "Epoch 90/100, Training Loss: 0.1413, Validation Loss: 0.3832\n",
      "Epoch 91/100, Training Loss: 0.1381, Validation Loss: 0.3783\n",
      "Epoch 92/100, Training Loss: 0.1378, Validation Loss: 0.3905\n",
      "Epoch 93/100, Training Loss: 0.1348, Validation Loss: 0.3699\n",
      "Epoch 94/100, Training Loss: 0.1312, Validation Loss: 0.3748\n",
      "Epoch 95/100, Training Loss: 0.1324, Validation Loss: 0.3738\n",
      "Epoch 96/100, Training Loss: 0.1294, Validation Loss: 0.3622\n",
      "Epoch 97/100, Training Loss: 0.1256, Validation Loss: 0.3769\n",
      "Epoch 98/100, Training Loss: 0.1278, Validation Loss: 0.3671\n",
      "Epoch 99/100, Training Loss: 0.1235, Validation Loss: 0.3766\n",
      "Epoch 100/100, Training Loss: 0.1216, Validation Loss: 0.3687\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        # images, labels = images.to(\"cuda\"), labels.to(\"cuda\") #for PC\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\") #for laptop\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            # images, labels = images.to(\"cuda\"), labels.to(\"cuda\")  # Use \"cuda\" if available\n",
    "            images, labels = images.to(\"cpu\"), labels.to(\"cpu\")  # Use \"cuda\" if available\n",
    "\n",
    "            # Forward pass on validation data\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "\n",
    "            # Accumulate validation loss\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    # Calculate average validation loss for this epoch\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # plot the loss of training and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f342542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:  tensor([ 7.6915, -3.9909, -2.4412,  1.3584, -2.1371,  0.3975, -2.2771])\n",
      "Predicted:  tensor([0, 1, 3, 3, 2])\n",
      "Actual:  tensor([0, 1, 3, 3, 2])\n",
      "Outputs:  tensor([ 2.7887, -1.5937,  2.4797, -2.6440, -2.6509,  4.9346, -3.7905])\n",
      "Predicted:  tensor([5, 1, 1, 0, 1])\n",
      "Actual:  tensor([5, 1, 1, 0, 3])\n",
      "Outputs:  tensor([ 3.5777,  1.1832, -3.1422, -0.1974,  0.8015,  0.3301, -3.9841])\n",
      "Predicted:  tensor([0, 5, 3, 0])\n",
      "Actual:  tensor([0, 5, 3, 0])\n",
      "Test Loss: 0.2783, Test Accuracy: 0.9118\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(\"cpu\"), labels.to(\"cpu\") #cuda for PC\n",
    "        # images, labels = images.to(\"cuda\"), labels.to(\"cuda\") #cuda for PC\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = loss_func(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        print(\"Outputs: \", outputs[0])\n",
    "        print(\"Predicted: \", predicted[:5])\n",
    "        print(\"Actual: \", labels[:5])\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "test_accuracy = correct_predictions/total_predictions\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
